{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["#**Helper & unused functions**"],"metadata":{"id":"k3X6lic6-xzB"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import cv2\n","from sklearn.model_selection import train_test_split\n","\n","from keras.layers import *\n","from keras.callbacks import EarlyStopping\n","from keras.utils import to_categorical\n","from keras.models import Model, Sequential\n","from keras.metrics import *\n","from keras.optimizers import Adam, RMSprop\n","from scipy.stats import norm\n","from keras.preprocessing import image\n","\n","from keras import backend as K"],"metadata":{"id":"MJOmg5jpggcy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import random\n","import shutil\n","\n","# Define the source folder containing files to split\n","source_folder = '/content/drive/MyDrive/VAE/cyberkongz'\n","\n","# Define the destination folders to store the split files\n","folder1 = '/content/drive/MyDrive/VAE/train'\n","folder2 = '/content/drive/MyDrive/VAE/validation'\n","\n","# Define the ratio of files to split between the two destination folders\n","split_ratio = 0.8\n","\n","# Get a list of all files in the source folder\n","files = os.listdir(source_folder)\n","\n","# Shuffle the file list to ensure randomness in splitting\n","random.shuffle(files)\n","\n","# Calculate the split index based on the split ratio\n","split_index = int(len(files) * split_ratio)\n","\n","# Copy the files up to the split index to folder1\n","for file in files[:split_index]:\n","    source_file = os.path.join(source_folder, file)\n","    dest_file = os.path.join(folder1, file)\n","    shutil.copyfile(source_file, dest_file)\n","\n","# Copy the files after the split index to folder2\n","for file in files[split_index:]:\n","    source_file = os.path.join(source_folder, file)\n","    dest_file = os.path.join(folder2, file)\n","    shutil.copyfile(source_file, dest_file)\n"],"metadata":{"id":"CMa6sGzKnUT7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","data_dir = '/content/drive/MyDrive/VAE/cyberkongz'\n","img_height = 680\n","img_width = 680\n","batch_size = 16\n","\n","train_ds = tf.keras.utils.image_dataset_from_directory(\n","  '/content/drive/MyDrive/VAE/train',\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size,\n","  labels=None)\n","\n","val_ds = tf.keras.utils.image_dataset_from_directory(\n","  '/content/drive/MyDrive/VAE/validation',\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size,\n","  labels=None)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4KNrnLK8ziAk","executionInfo":{"status":"ok","timestamp":1677172872228,"user_tz":-120,"elapsed":4458,"user":{"displayName":"Danu intern","userId":"14685699878604878640"}},"outputId":"c7ccec74-2ab2-4d65-9a9d-1d5b300f5797"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1260 files belonging to 1 classes.\n","Found 315 files belonging to 1 classes.\n"]}]},{"cell_type":"code","source":["import cv2\n","from google.colab.patches import cv2_imshow\n","from PIL import Image\n","import os \n","\n","!mkdir /content/cyberkongz_resized/\n","dir_path = '/content/drive/MyDrive/VAE/cyberkongz'\n","dir2_path = '/content/cyberkongz_resized'\n","for path in os.listdir(dir_path):\n","  complete_path = os.path.join(dir_path, path)\n","  img = cv2.imread(complete_path, cv2.IMREAD_UNCHANGED)\n","  resized = cv2.resize(img, (512,512), interpolation = Image.ANTIALIAS )  #cv2.INTER_AREA\n","  #cv2_imshow(resized)\n","  new_complete_path = os.path.join(dir2_path, path)\n","  cv2.imwrite(new_complete_path,resized)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Xrl-mOKUZSr","executionInfo":{"status":"ok","timestamp":1677269148151,"user_tz":-120,"elapsed":65728,"user":{"displayName":"Danu intern","userId":"14685699878604878640"}},"outputId":"634ba1cb-846d-4c8a-9ad9-90df5a0959c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/content/cyberkongz_resized/’: File exists\n"]}]},{"cell_type":"code","source":["!cp -r /content/cyberkongz_resized /content/drive/MyDrive/VAE"],"metadata":{"id":"CsJZDVQVUx4Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**test**"],"metadata":{"id":"pu5QXcP0POyx"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, Flatten, Lambda, Reshape\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import backend as K\n","\n","def build_vae(input_shape, latent_dim, filters):\n","    width, height, channels = input_shape\n","\n","    # Define encoder layers\n","    encoder_input = Input(shape=input_shape)\n","    x = encoder_input\n","    for i, f in enumerate(filters):\n","        x = Conv2D(filters=f, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\", name=f\"encoder_conv{i}\")(x)\n","    x = Flatten()(x)\n","    z_mean = Dense(units=latent_dim, name=\"z_mean\")(x)\n","    z_log_var = Dense(units=latent_dim, name=\"z_log_var\")(x)\n","\n","    # Define sampling layer\n","    def sampling(args):\n","        z_mean, z_log_var = args\n","        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.0, stddev=1.0)\n","        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n","\n","    z = Lambda(sampling, name=\"z\")([z_mean, z_log_var])\n","\n","    # Define decoder layers\n","    decoder_input = Input(shape=(latent_dim,))\n","    x = decoder_input\n","    x = Dense(units=(width // 16) * (height // 16) * filters[-1], name=\"decoder_dense\")(x)\n","    x = Reshape(target_shape=((width // 16), (height // 16), filters[-1]), name=\"decoder_reshape\")(x)\n","    for i, f in reversed(list(enumerate(filters[:-1]))):\n","        x = Conv2DTranspose(filters=f, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\", name=f\"decoder_conv{i}\")(x)\n","    x = Conv2DTranspose(filters=channels, kernel_size=3, strides=2, activation=\"sigmoid\", padding=\"same\", name=\"decoder_output\")(x)\n","\n","    # Define models\n","    encoder = Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\")\n","    decoder = Model(decoder_input, x, name=\"decoder\")\n","    vae_output = decoder(z)\n","    vae = Model(encoder_input, vae_output, name=\"vae\")\n","\n","    # Define loss function\n","    def vae_loss(x, x_recon):\n","        reconstruction_loss = K.sum(K.square(x - x_recon), axis=[1, 2, 3])\n","        kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n","        return K.mean(reconstruction_loss + kl_loss)\n","\n","    vae.compile(optimizer='Adam', loss=vae_loss)\n","\n","    return vae\n"],"metadata":{"id":"FfgEaYsGPRch"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","# Load the image dataset\n","data_dir = \"/content/drive/MyDrive/VAE/train\"\n","img_height = 512\n","img_width = 512\n","batch_size = 1\n","\n","train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    data_dir,\n","    validation_split=0.2,\n","    subset=\"training\",\n","    seed=123,\n","    image_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    labels=None)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZKo-UDy-0OTZ","executionInfo":{"status":"ok","timestamp":1677483767295,"user_tz":-120,"elapsed":1329,"user":{"displayName":"Danu intern","userId":"14685699878604878640"}},"outputId":"e51a6b43-d7fc-413c-e923-ba2e70a6b978"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5000 files belonging to 1 classes.\n","Using 4000 files for training.\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","# Load and preprocess data\n","train_data = ... # load and preprocess your dataset here\n","test_data = ... # load and preprocess your dataset here\n","\n","# Define model\n","input_shape = (512, 512, 3)\n","latent_dim = 256\n","filters = [32, 64, 128, 256]\n","vae = build_vae(input_shape, latent_dim, filters)\n","\n","# Train model\n","batch_size = 32\n","epochs = 10\n","\n","for epoch in range(epochs):\n","    print(f\"Epoch {epoch+1}/{epochs}\")\n","    for step, x_batch in enumerate(train_ds.batch(batch_size)):\n","        loss = vae.train_on_batch(x_batch, x_batch)\n","        if step % 100 == 0:\n","            print(f\"Step {step}: loss = {loss}\")\n","    \n","    # Evaluate on test data\n","    test_loss = vae.evaluate(test_data, test_data, batch_size=batch_size, verbose=0)\n","    print(f\"Test loss: {test_loss:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":742},"id":"c6zdKCASPUvp","outputId":"a31409d0-07d0-4aa0-99f0-94eac68f4293","executionInfo":{"status":"error","timestamp":1677483772724,"user_tz":-120,"elapsed":703,"user":{"displayName":"Danu intern","userId":"14685699878604878640"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-6ad622b0ee74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Step {step}: loss = {loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2476\u001b[0m             )\n\u001b[1;32m   2477\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2478\u001b[0;31m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1231\u001b[0m                 )\n\u001b[1;32m   1232\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m             outputs = reduce_per_replica(\n\u001b[1;32m   1235\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m                 \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;31m# Run forward pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_target_and_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                         raise ValueError(\n\u001b[0m\u001b[1;32m    296\u001b[0m                             \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                             \u001b[0;34m\"incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"vae\" is incompatible with the layer: expected shape=(None, 512, 512, 3), found shape=(32, 1, 512, 512, 3)\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, Flatten, Lambda\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import backend as K\n","\n","def build_vae(input_shape):\n","    width, height, channels = input_shape\n","\n","    # Define encoder layers\n","    encoder_input = Input(shape=input_shape)\n","    x = Conv2D(filters=32 * (width // 64), kernel_size=3, strides=2, activation=\"relu\", padding=\"same\")(encoder_input)\n","    x = Conv2D(filters=64 * (width // 64), kernel_size=3, strides=2, activation=\"relu\", padding=\"same\")(x)\n","    x = Conv2D(filters=128 * (width // 64), kernel_size=3, strides=2, activation=\"relu\", padding=\"same\")(x)\n","    x = Conv2D(filters=256 * (width // 64), kernel_size=3, strides=2, activation=\"relu\", padding=\"same\")(x)\n","    x = Flatten()(x)\n","    z_mean = Dense(units=latent_dim)(x)\n","    z_log_var = Dense(units=latent_dim)(x)\n","\n","    # Define sampling layer\n","    def sampling(args):\n","        z_mean, z_log_var = args\n","        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.0, stddev=1.0)\n","        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n","\n","    z = Lambda(sampling)([z_mean, z_log_var])\n","\n","    # Define decoder layers\n","    decoder_input = Input(shape=(latent_dim,))\n","    x = Dense(units=(width // 16) * (height // 16) * 256)(decoder_input)\n","    x = Reshape(target_shape=((width // 16), (height // 16), 256))(x)\n","    x = Conv2DTranspose(filters=128 * (width // 64), kernel_size=3, strides=2, activation=\"relu\", padding=\"same\")(x)\n","    x = Conv2DTranspose(filters=64 * (width // 64), kernel_size=3, strides=2, activation=\"relu\", padding=\"same\")(x)\n","    x = Conv2DTranspose(filters=32 * (width // 64), kernel_size=3, strides=2, activation=\"relu\", padding=\"same\")(x)\n","    x = Conv2DTranspose(filters=channels, kernel_size=3, strides=2, activation=\"sigmoid\", padding=\"same\")(x)\n","\n","    # Define models\n","    encoder = Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\")\n","    decoder = Model(decoder_input, x, name=\"decoder\")\n","    vae_output = decoder(z)\n","    vae = Model(encoder_input, vae_output, name=\"vae\")\n","\n","    # Define loss function\n","    def vae_loss(x, x_recon):\n","        reconstruction_loss = K.sum(K.square(x - x_recon), axis=[1, 2, 3])\n","        kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n","        return K.mean(reconstruction_loss + kl_loss)\n","\n","    vae.compile(optimizer=optimizer, loss=vae_loss)\n","\n","    return vae\n"],"metadata":{"id":"z4X72Br6BvMU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Load Data & Model Construction**"],"metadata":{"id":"wMPEcNH8_bDp"}},{"cell_type":"code","source":["img = cv2.imread('/content/drive/MyDrive/VAE/cyberkongz/1.png')\n","dimensions = img.shape\n","dimensions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5t9lfm7iUN5","executionInfo":{"status":"ok","timestamp":1677160246763,"user_tz":-120,"elapsed":296,"user":{"displayName":"Danu intern","userId":"14685699878604878640"}},"outputId":"6880d720-5043-4aa5-d22f-07dfc66df5c4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(680, 680, 3)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import os\n","\n","# Load the image dataset\n","data_dir = \"/content/drive/MyDrive/VAE/train\"\n","img_height = 512\n","img_width = 512\n","batch_size = 4\n","\n","train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    data_dir,\n","    validation_split=0.2,\n","    subset=\"training\",\n","    seed=123,\n","    image_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    labels=None)\n","\n","def normalize(x):\n","    x = tf.cast(x, tf.float32)\n","    x = x / 255.0\n","    x = (x - 0.5) * 2.0\n","    return x\n","\n","train_ds = train_ds.map(normalize)\n","\n","#train_ds = train_ds.map(lambda x: x/255.0)\n","\n","#train_ds = train_ds.map(lambda x: (tf.image.resize(x, (img_height, img_width)) - 0.5) * 2)\n","\n","train_ds = train_ds.shuffle(buffer_size=1000)\n","train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n","\n","# Preprocess the images\n","#train_ds = train_ds.map(lambda x: tf.image.resize(x, (img_height, img_width))/255.0,)\n","\n","# Prepare the data\n","#train_ds = train_ds.map(lambda x: (x, x)) # VAE requires input and output to be same\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-mC3OpX95sck","executionInfo":{"status":"ok","timestamp":1677485755140,"user_tz":-120,"elapsed":2938,"user":{"displayName":"Danu intern","userId":"14685699878604878640"}},"outputId":"6bac65fe-6f71-4242-88a2-d631244fd3f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5000 files belonging to 1 classes.\n","Using 4000 files for training.\n"]}]},{"cell_type":"code","source":["min_val = 1.0\n","max_val = 0.0\n","for x in train_ds:\n","    min_val = tf.reduce_min(x).numpy()\n","    max_val = tf.reduce_max(x).numpy()\n","    \n","print(f\"Minimum pixel value: {min_val:.4f}\")\n","print(f\"Maximum pixel value: {max_val:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6yr6WP2T1pLC","executionInfo":{"status":"ok","timestamp":1677450755736,"user_tz":-120,"elapsed":7840,"user":{"displayName":"Danu intern","userId":"14685699878604878640"}},"outputId":"53901c06-1bb5-4db4-b05a-22304601265a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Minimum pixel value: -1.0000\n","Maximum pixel value: 1.0000\n"]}]},{"cell_type":"code","source":["import tensorflow.keras.backend as K\n","import tensorflow as tf\n","latent_dim = 100\n","\n","encoder_inputs = tf.keras.layers.Input(shape=(img_height, img_width, 3))\n","x = tf.keras.layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(encoder_inputs)\n","x = tf.keras.layers.Dropout(0.2)(x)\n","x = tf.keras.layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n","x = tf.keras.layers.Dropout(0.2)(x)\n","x = tf.keras.layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n","x = tf.keras.layers.Dropout(0.2)(x)\n","x = tf.keras.layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n","x = tf.keras.layers.Dropout(0.2)(x)\n","x = tf.keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n","x = tf.keras.layers.Dropout(0.2)(x)\n","x = tf.keras.layers.Conv2D(128, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n","x = tf.keras.layers.Dropout(0.2)(x)\n","x = tf.keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\")(x)\n","x = tf.keras.layers.Dropout(0.2)(x)\n","x = tf.keras.layers.Flatten()(x)\n","\n","x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n","x = tf.keras.layers.Dropout(0.2)(x)\n","x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n","x = tf.keras.layers.Dropout(0.2)(x)\n","\n","x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n","x = tf.keras.layers.Dropout(0.2)(x)\n","\n","z_mean = tf.keras.layers.Dense(latent_dim, name=\"z_mean\")(x)\n","z_log_var = tf.keras.layers.Dense(latent_dim, name=\"z_log_var\")(x)\n","\n","encoder = tf.keras.Model(encoder_inputs, [z_mean, z_log_var], name=\"encoder\")\n","\n","latent_inputs = tf.keras.layers.Input(shape=(latent_dim,))\n","x = tf.keras.layers.Dense(256, activation=\"relu\")(latent_inputs)\n","#x = tf.keras.layers.Dense(512 * 512 * 32, activation=\"relu\")(x)\n","x = tf.keras.layers.Dense(256 * 256 * 32, activation=\"relu\")(x)\n","x = tf.keras.layers.Reshape((256, 256, 32))(x)\n","\n","x = tf.keras.layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\")(x)\n","x = tf.keras.layers.Dropout(0.2)(x)\n","x = tf.keras.layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n","x = tf.keras.layers.Dropout(0.2)(x)\n","decoder_outputs = tf.keras.layers.Conv2DTranspose(3, 3, activation=\"sigmoid\", padding=\"same\")(x)\n","\n","decoder = tf.keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n","\n","\n","class VAE(tf.keras.Model):\n","    def __init__(self, encoder, decoder, **kwargs):\n","        super(VAE, self).__init__(**kwargs)\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def sample(self, eps=None):\n","        if eps is None:\n","            eps = tf.random.normal(shape=(100,))\n","        return self.decode(eps, apply_sigmoid=True)\n","\n","    def sampling(self,z_mean,z_log_var):\n","        #z_mean, z_log_var = args\n","        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.0, stddev=1.0)\n","        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n","\n","    def encode(self, x):\n","        mean, logvar = self.encoder(x)\n","        return mean, logvar\n","\n","    def reparameterize(self, mean, logvar):\n","        eps = tf.random.normal(shape=mean.shape)\n","        return eps * tf.exp(logvar * 0.5) + mean\n","\n","    def decode(self, z, apply_sigmoid=False):\n","        logits = self.decoder(z)\n","        if apply_sigmoid:\n","            probs = tf.sigmoid(logits)\n","            return probs\n","        return logits\n","\n","# Define the VAE model\n","vae = VAE(encoder, decoder)\n","\n","# Define the loss function\n","def vae_loss(encoder, decoder, x, z_mean, z_log_var):\n","    reconstruction_loss = tf.reduce_mean(tf.square(x - decoder(z_mean)))"],"metadata":{"id":"h_SKmGd1MRw8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = tf.keras.optimizers.Adam(learning_rate=1e-6)\n","epochs = 5\n","for epoch in range(epochs):\n","    print(f\"Epoch {epoch+1}/{epochs}\")\n","    kl_losses = []\n","    for step, x in enumerate(train_ds):\n","        with tf.GradientTape() as tape:\n","            z_mean, z_log_var = vae.encode(x)\n","            z = vae.reparameterize(z_mean, z_log_var)\n","            x_recon = vae.decode(z)\n","\n","            #print(x.shape)\n","            #print(x_recon.shape)\n","\n","            # Compute reconstruction loss\n","            reconstruction_loss = tf.reduce_mean(tf.square(x - x_recon))\n","            #reconstruction_loss *= 255.0**2  # Scale by factor to account for normalization\n","\n","            # Compute KL divergence loss with weight decay\n","            weight_decay = 0.1\n","            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n","                              \n","            # Compute total loss\n","            beta = 0.1 * epoch / epochs  # Increase beta gradually during training\n","            total_loss = reconstruction_loss + weight_decay * kl_loss\n","\n","\n","        # Compute gradients and update weights\n","        grads = tape.gradient(total_loss, vae.trainable_weights)\n","        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n","\n","        # Log progress\n","        if step % 100 == 0:\n","            print(f\"Step {step}: Reconstruction loss = {reconstruction_loss.numpy():.4f}, KL loss = {kl_loss.numpy():.4f}, Total loss = {total_loss.numpy():.4f}\")\n","\n","        kl_losses.append(kl_loss)\n","\n","    # Calculate and print the average KL loss for the epoch\n","    avg_kl_loss = tf.reduce_mean(kl_losses)\n","    print(f\"Epoch {epoch+1} average KL loss: {avg_kl_loss.numpy():.4f}\")\n","\n","    # Save model weights every epoch\n","    vae.save_weights(\"vae_weights.h5\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nl1CEley2eNB","executionInfo":{"status":"ok","timestamp":1677484935479,"user_tz":-120,"elapsed":405234,"user":{"displayName":"Danu intern","userId":"14685699878604878640"}},"outputId":"480d10fc-cf68-41fd-c032-57b7db43f1ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f7868623700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f7868623700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["Step 0: Reconstruction loss = 0.8903, KL loss = 0.0000, Total loss = 0.8903\n","Step 100: Reconstruction loss = 0.9893, KL loss = 0.0000, Total loss = 0.9893\n","Step 200: Reconstruction loss = 0.7823, KL loss = 0.0000, Total loss = 0.7823\n","Step 300: Reconstruction loss = 0.8213, KL loss = 0.0000, Total loss = 0.8213\n","Step 400: Reconstruction loss = 0.8572, KL loss = 0.0000, Total loss = 0.8572\n","Step 500: Reconstruction loss = 0.9186, KL loss = 0.0000, Total loss = 0.9186\n","Step 600: Reconstruction loss = 0.8574, KL loss = 0.0000, Total loss = 0.8574\n","Step 700: Reconstruction loss = 0.9605, KL loss = 0.0000, Total loss = 0.9605\n","Step 800: Reconstruction loss = 0.7642, KL loss = 0.0000, Total loss = 0.7642\n","Step 900: Reconstruction loss = 0.8421, KL loss = 0.0000, Total loss = 0.8421\n","Epoch 1 average KL loss: 0.0000\n","Epoch 2/5\n","Step 0: Reconstruction loss = 0.8886, KL loss = 0.0000, Total loss = 0.8886\n","Step 100: Reconstruction loss = 0.9873, KL loss = 0.0000, Total loss = 0.9873\n","Step 200: Reconstruction loss = 0.7806, KL loss = 0.0000, Total loss = 0.7806\n","Step 300: Reconstruction loss = 0.8190, KL loss = 0.0000, Total loss = 0.8190\n","Step 400: Reconstruction loss = 0.8545, KL loss = 0.0000, Total loss = 0.8545\n","Step 500: Reconstruction loss = 0.9154, KL loss = 0.0000, Total loss = 0.9154\n","Step 600: Reconstruction loss = 0.8537, KL loss = 0.0000, Total loss = 0.8537\n","Step 700: Reconstruction loss = 0.9558, KL loss = 0.0000, Total loss = 0.9558\n","Step 800: Reconstruction loss = 0.7599, KL loss = 0.0001, Total loss = 0.7599\n","Step 900: Reconstruction loss = 0.8370, KL loss = 0.0001, Total loss = 0.8370\n","Epoch 2 average KL loss: 0.0000\n","Epoch 3/5\n","Step 0: Reconstruction loss = 0.8804, KL loss = 0.0002, Total loss = 0.8804\n","Step 100: Reconstruction loss = 0.9783, KL loss = 0.0004, Total loss = 0.9783\n","Step 200: Reconstruction loss = 0.7724, KL loss = 0.0005, Total loss = 0.7725\n","Step 300: Reconstruction loss = 0.8074, KL loss = 0.0008, Total loss = 0.8075\n","Step 400: Reconstruction loss = 0.8411, KL loss = 0.0014, Total loss = 0.8412\n","Step 500: Reconstruction loss = 0.9007, KL loss = 0.0019, Total loss = 0.9008\n","Step 600: Reconstruction loss = 0.8359, KL loss = 0.0033, Total loss = 0.8363\n","Step 700: Reconstruction loss = 0.9352, KL loss = 0.0045, Total loss = 0.9356\n","Step 800: Reconstruction loss = 0.7426, KL loss = 0.0051, Total loss = 0.7431\n","Step 900: Reconstruction loss = 0.8150, KL loss = 0.0090, Total loss = 0.8159\n","Epoch 3 average KL loss: 0.0037\n","Epoch 4/5\n","Step 0: Reconstruction loss = 0.8517, KL loss = 0.0171, Total loss = 0.8534\n","Step 100: Reconstruction loss = 0.9348, KL loss = 0.0308, Total loss = 0.9379\n","Step 200: Reconstruction loss = 0.7356, KL loss = 0.0325, Total loss = 0.7389\n","Step 300: Reconstruction loss = 0.7543, KL loss = 0.0521, Total loss = 0.7595\n","Step 400: Reconstruction loss = 0.7812, KL loss = 0.0871, Total loss = 0.7899\n","Step 500: Reconstruction loss = 0.8362, KL loss = 0.1231, Total loss = 0.8485\n","Step 600: Reconstruction loss = 0.7570, KL loss = 0.1598, Total loss = 0.7730\n","Step 700: Reconstruction loss = 0.8487, KL loss = 0.2315, Total loss = 0.8718\n","Step 800: Reconstruction loss = 0.6646, KL loss = 0.1797, Total loss = 0.6825\n","Step 900: Reconstruction loss = 0.6997, KL loss = 0.2501, Total loss = 0.7247\n","Epoch 4 average KL loss: 0.1395\n","Epoch 5/5\n","Step 0: Reconstruction loss = 0.7015, KL loss = 0.3787, Total loss = 0.7393\n","Step 100: Reconstruction loss = 0.7609, KL loss = 0.4780, Total loss = 0.8087\n","Step 200: Reconstruction loss = 0.6173, KL loss = 0.2943, Total loss = 0.6467\n","Step 300: Reconstruction loss = 0.6120, KL loss = 0.3859, Total loss = 0.6506\n","Step 400: Reconstruction loss = 0.6220, KL loss = 0.4296, Total loss = 0.6650\n","Step 500: Reconstruction loss = 0.6638, KL loss = 0.4492, Total loss = 0.7087\n","Step 600: Reconstruction loss = 0.5983, KL loss = 0.4220, Total loss = 0.6405\n","Step 700: Reconstruction loss = 0.6546, KL loss = 0.4785, Total loss = 0.7025\n","Step 800: Reconstruction loss = 0.5578, KL loss = 0.2711, Total loss = 0.5849\n","Step 900: Reconstruction loss = 0.6006, KL loss = 0.3599, Total loss = 0.6366\n","Epoch 5 average KL loss: 0.3995\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","from PIL import Image\n","\n","# Load your trained VAE model here\n","vae = ...\n","\n","# Load your input image here using a suitable image loading library\n","image = Image.open(\"/content/drive/MyDrive/VAE/train/10.png\")#.resize((64, 64))\n","\n","# Convert the image to a tensor\n","image_tensor = tf.convert_to_tensor(np.array(image))\n","#\n","image_tensor = tf.cast(image_tensor, tf.float32)\n","\n","#normalize photo \n","image_tensor = tf.map_fn(normalize,image_tensor)\n","\n","# Add a batch dimension to the tensor\n","image_tensor = tf.expand_dims(image_tensor, axis=0)\n","\n","# Call vae.encode() on the input tensor to obtain mean and log variance of latent space\n","z_mean, z_log_var = vae.encode(image_tensor.numpy())\n","\n","# Use mean and log variance to generate latent space vector using vae.reparameterize()\n","z = vae.reparameterize(z_mean, z_log_var)\n","\n","# Call vae.decode() on latent space vector to generate reconstructed image\n","reconstructed_image = vae.decode(z)\n","\n","# Convert reconstructed image tensor to a numpy array\n","reconstructed_image_array = reconstructed_image.numpy()\n","\n","# Visualize input image and reconstructed image side-by-side\n","import matplotlib.pyplot as plt\n","fig, axes = plt.subplots(nrows=1, ncols=2)\n","axes[0].imshow(image)\n","axes[0].set_title(\"Input Image\")\n","axes[1].imshow(reconstructed_image_array[0])\n","axes[1].set_title(\"Reconstructed Image\")\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":261},"id":"YW6bn9Ec3wa9","executionInfo":{"status":"error","timestamp":1677485799007,"user_tz":-120,"elapsed":741,"user":{"displayName":"Danu intern","userId":"14685699878604878640"}},"outputId":"1babaa46-0209-48be-fdaa-750fa7a5caa0"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-87ffb34e4600>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Call vae.encode() on the input tensor to obtain mean and log variance of latent space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Use mean and log variance to generate latent space vector using vae.reparameterize()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'encode'"]}]},{"cell_type":"code","source":["filepath = '/content/drive/MyDrive/VAE/train/1.png'\n","image = cv2.imread(filepath)\n","image = asarray(image)\n","image = np.expand_dims(image, axis=0)\n","z_mean,z_log_var = vae.encode(image)\n","#print(z_mean,z_log_var)"],"metadata":{"id":"ibSElN9LoqK4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filepath = '/content/drive/MyDrive/VAE/train/5.png'\n","image = cv2.imread(filepath)\n","image = asarray(image)\n","image = np.expand_dims(image, axis=0)\n","z_mean1,z_log_var1 = vae.encode(image)"],"metadata":{"id":"fqaCdsxhoysw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","tf.math.equal(z_mean,z_mean1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LUz05NJ-o4BW","executionInfo":{"status":"ok","timestamp":1677452503097,"user_tz":-120,"elapsed":5,"user":{"displayName":"Danu intern","userId":"14685699878604878640"}},"outputId":"5937a657-a377-4309-c202-85695041c06b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 100), dtype=bool, numpy=\n","array([[False, False, False, False, False, False, False, False, False,\n","        False, False, False, False, False, False, False, False, False,\n","        False, False, False, False, False, False, False, False, False,\n","        False, False, False, False, False, False, False, False, False,\n","        False, False, False, False, False, False, False, False, False,\n","        False, False, False, False, False, False, False, False, False,\n","        False, False, False, False, False, False, False, False, False,\n","        False, False, False, False, False, False, False, False, False,\n","        False, False, False, False, False, False, False, False, False,\n","        False, False, False, False, False, False, False, False, False,\n","        False, False, False, False, False, False, False, False, False,\n","        False]])>"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["import os\n","import cv2\n","from numpy import asarray\n","\n","directory = '/content/drive/MyDrive/VAE/train'\n","i = 0\n","for filename in os.listdir(directory):\n","    if filename.endswith('.png') and i < 5:  # only process .txt files\n","        filepath = os.path.join(directory, filename)\n","        image = cv2.imread(filepath)\n","        image = asarray(image)\n","        image = np.expand_dims(image, axis=0)\n","        z_mean,z_log_var = vae.encode(image)\n","        print(z_mean,z_log_var)\n","        i = i+1\n","    else:\n","      continue\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iac4zdbsjhVl","executionInfo":{"status":"ok","timestamp":1677452089043,"user_tz":-120,"elapsed":1777,"user":{"displayName":"Danu intern","userId":"14685699878604878640"}},"outputId":"4369adf3-3dfb-4b45-d72f-ece0ac47d600"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[ -2.0033805   17.080898    -1.2972615    0.6946153   42.850464\n","  -17.846684   -22.432886   -39.693752   -16.073637   -38.588314\n","   -4.6271667    6.946963     0.41680136  23.577217   -12.020315\n","  -13.338169    14.808459     5.3561153   24.368383    43.657448\n","   31.696087    38.267162   -20.322063     2.8362198   -8.464914\n","   29.996197    -6.46824     -5.455508   -66.50456     41.02135\n","   21.2962     -39.56283    -16.396925    74.125496     3.564946\n","   -4.2043786  -14.590085   -51.189846     7.3757915   33.475266\n","   19.19889    -17.283398    11.663569    40.85026    -44.055843\n","   18.317533    -5.747716     8.47699     21.63666      8.281494\n","    5.411768   -23.097197   -12.7886505   -6.181515   -23.170536\n","   -8.171186    14.837403    16.134438   -10.210181    24.325218\n","   -0.55579317   6.971418   -23.775484   -46.017097   -38.902187\n","   -8.248863    35.389267    -3.739119    -8.998779    15.531243\n","   22.274834     8.296277   -18.661232   -18.370853    26.866049\n","  -52.223984   -34.81809    -37.92199      8.04048      0.21409799\n","   45.28467     29.825954    32.63233     -7.5868654  -29.910099\n","  -20.407919     6.944659   -17.75281      7.585197   -16.42889\n","   28.922913    14.05313    -43.220627     7.8478165   21.555517\n","   22.92072     43.43901     15.568374   -31.533728   -39.695892  ]], shape=(1, 100), dtype=float32) tf.Tensor(\n","[[  2.8956501 -19.53826     4.209718  -37.923214   51.94072    11.013069\n","  -25.507862   -5.676765    2.6977713  11.964652    1.7352127  -7.4444466\n","    3.9218457  -5.070872  -22.746597   -9.84447     3.751732   17.937237\n","   15.029459    7.3052144  29.15132   -20.15188    16.048908   17.969555\n","  -18.81951    35.9449    -14.574892   37.93594    20.934488   26.858671\n","    5.1304917  26.620281  -11.766352  -48.694317    8.784422   19.390959\n","   29.937542  -15.054758   -2.7324123 -14.271055    6.1913757 -26.340343\n","   54.764862    5.596656   22.991764   30.676775   -6.1748924 -19.994484\n","    4.9824734   5.553484   32.989674   -4.68221    16.87455   -27.858698\n","  -12.930238    0.6153779 -16.303856  -33.535503   11.7893715  36.31265\n","  -23.011883  -22.78241    13.536557  -24.458372   40.12269    12.738493\n","   -3.4016283   3.8094738  16.88844   -21.528297   -8.9855     12.880228\n","   16.233885   -6.5752506  -9.491326  -27.629936  -22.363316  -19.623192\n","  -11.362068  -31.013329  -33.409653   25.20547    20.326403   18.437962\n","  -27.890623  -24.797337   23.917871  -22.549322    8.218624    2.0425205\n","   18.371607    5.6567335  -7.630032    3.8509758 -12.367447   32.891945\n","  -27.205963   30.072905   49.236782   26.410961 ]], shape=(1, 100), dtype=float32)\n","tf.Tensor(\n","[[ -6.0755944   14.643837     0.14576982  -4.7170258   44.358643\n","  -16.935091   -21.34491    -57.3765      -4.496566   -48.349957\n","    6.240576    -2.730785     7.670184    30.842464   -19.203423\n","  -18.786448    20.242846    10.326648    24.905987    28.043583\n","   29.089394    42.687557   -12.318652     4.5065145  -13.7405615\n","   35.52893     -0.5220652  -20.56842    -76.883606    39.95809\n","   13.321786   -44.01257    -19.22247     64.49408     14.385951\n","  -10.662834   -16.643597   -49.96963     14.913815    51.68911\n","    9.147461   -12.926352     8.938986    41.24278    -49.525093\n","   14.90044     -1.2838678    0.6822428   12.6888685    0.6324942\n","   -2.160494   -17.414074    -6.2941985   11.404485   -36.58916\n","   -9.147906    20.698385    19.379326   -21.126299    18.751549\n","   11.811253    13.846401   -21.28668    -59.704784   -42.269627\n","   -0.55172443  36.03509     11.157782     1.7648892   30.092293\n","   40.6534       3.8722045  -28.604673   -39.92634     12.610212\n","  -59.45663    -45.11018    -47.408516    -7.8255596   -3.4657888\n","   62.80325     22.140972    52.583534    -4.031679   -37.147865\n","  -33.81247     21.376736   -13.353874     5.220404   -18.283272\n","   29.756819     6.133811   -52.509758    17.374855    22.491323\n","   12.753083    48.938786    12.041201   -44.67984    -58.1089    ]], shape=(1, 100), dtype=float32) tf.Tensor(\n","[[ 15.271325  -16.806923    4.4903655 -48.120502   72.05472    24.64733\n","  -19.22336   -11.588579    9.862113   13.957154   -6.316334   -6.1846433\n","    2.6724412   4.9134603 -23.928059  -16.455801   16.847948   19.665077\n","   19.699923    3.194451   15.21231   -22.505533    6.9372625  20.06841\n","  -15.791783   40.308456  -11.237412   37.568264   37.59691    34.534866\n","    8.163665   32.21756    -7.7196646 -55.409557   11.585487   26.219446\n","   20.720621  -22.379946   -1.7210857 -15.060741    7.357888  -19.878082\n","   61.908535    4.7779636  27.868385   53.14874   -11.756149  -26.36105\n","    2.708884    1.0178596  31.251059    3.5693295  30.605433  -29.57327\n","  -19.42886    18.31957   -12.83561   -42.014614   14.011551   35.93256\n","  -29.245457  -30.020485   30.642523  -36.062885   40.90783    12.03068\n","    1.4649494  19.765224   20.575216  -22.31508   -14.248205   10.620362\n","   15.044037    4.3206615 -19.03401   -20.596218  -22.332357  -12.72498\n","   -9.699264  -30.812523  -35.91827    27.727825   37.189407   26.081137\n","  -41.110268  -17.632303   44.32604   -27.88593    17.525078   13.725677\n","   19.540457   15.357359  -12.156233    4.062886  -19.898664   29.18563\n","  -36.061733   30.517612   65.88255    46.076717 ]], shape=(1, 100), dtype=float32)\n","tf.Tensor(\n","[[  2.807941    15.098581    -2.6798737    0.42472237  38.223408\n","  -13.045481   -21.156425   -34.529377   -17.555792   -40.774235\n","    1.8974886    1.4470488    2.3033736   22.290287   -11.806007\n","  -11.122015    12.06381      0.09693743  14.805541    41.08262\n","   23.586636    39.172455   -18.65857     -1.3837601  -12.670206\n","   31.920206    -3.8707695   -6.1875477  -61.056835    37.071766\n","   19.881454   -35.749588   -11.863787    71.31999      2.252252\n","   -3.1726484   -8.714178   -47.896015    10.34498     29.03215\n","   19.539019   -13.251819     5.8585253   33.68644    -40.22473\n","   14.030269    -3.3115041   12.62996     16.78254     10.217862\n","    6.9174023  -22.645294   -15.991102    -0.9250324  -18.613514\n","   -7.401607    14.203942    15.0213175   -5.8556895   18.890041\n","   -0.08830205   5.715429   -24.421347   -47.464558   -37.58046\n","   -0.24456622  33.897903   -12.900984     1.5923276   13.719511\n","   21.895391     9.007983   -14.167177   -12.392966    29.471704\n","  -45.073383   -24.89953    -29.839338     8.196449     3.4869394\n","   41.24095     26.213972    29.152426    -3.1170626  -25.36641\n","  -15.547884     4.940776   -14.523232     5.7159905  -15.981317\n","   26.0828       9.314118   -40.197826    10.359524    17.06829\n","   21.617638    35.628933    12.09632    -29.13124    -32.455933  ]], shape=(1, 100), dtype=float32) tf.Tensor(\n","[[  2.7766833  -19.268967     2.9080036  -29.216373    47.26843\n","    8.521496   -24.5159      -2.719982     1.4257991   10.66173\n","   -0.0828472   -8.213525     7.0145807   -2.6991184  -20.799122\n","  -13.887012     3.6085818   15.417912     5.2452526    5.268348\n","   24.68629    -20.159573    14.880365    13.764405   -17.171965\n","   27.212656   -10.6778345   37.86253     11.06869     22.96744\n","    4.249907    20.494411    -8.644944   -43.758686     2.8187265\n","   16.983215    28.100813   -10.56016     -0.68919337 -14.12291\n","    9.847294   -19.542599    44.933064     2.7356627   24.32326\n","   23.681883    -4.8348274  -16.898968     5.081807     5.8724165\n","   32.828373    -4.774703    20.040344   -24.10511    -10.430045\n","    1.8573904  -16.811287   -21.417292    12.804809    28.83164\n","  -26.241096   -22.361704     9.104626   -21.71207     29.493633\n","   11.247353    -8.312149     5.9930177   11.362611   -18.559484\n","   -3.4852133   10.696096    14.434888    -6.7457438   -4.1862397\n","  -20.970043   -23.466026   -14.093528    -5.566957   -24.621193\n","  -28.843119    20.162502    20.487865    12.737405   -23.690746\n","  -24.953592    19.753757   -13.387174     5.608088     1.5290699\n","   12.764807     2.9964113  -10.505813     4.4675417   -8.344472\n","   32.67618    -23.88609     29.618065    48.949753    22.64582   ]], shape=(1, 100), dtype=float32)\n","tf.Tensor(\n","[[  3.7485828   14.009594    -2.2304528   -1.3347508   36.72246\n","  -11.535973   -22.115993   -37.19143    -16.399792   -42.63032\n","    4.6486025   -0.31569996   3.6130152   21.933643   -11.726859\n","  -10.379988    13.163487    -1.0903075   12.161415    38.579742\n","   21.526905    38.38245    -17.085651    -2.087721   -12.493023\n","   33.65016     -2.0605192   -8.310088   -61.32671     36.78186\n","   17.903437   -35.473957   -12.16003     69.70126      3.6405365\n","   -4.599884    -8.202763   -47.72173     12.87413     31.97057\n","   17.92992    -11.881748     5.1037064   33.52303    -39.396553\n","   12.75927     -1.4971144   11.750511    13.526425     7.9664254\n","    5.9294853  -21.510567   -15.37921      1.7063506  -20.218004\n","   -7.0240874   14.031001    14.966079    -7.820315    16.209305\n","    2.4316154    5.5813336  -22.948212   -49.486      -36.239323\n","    1.8485754   34.443237   -11.717331     5.5753293   14.463472\n","   23.92838      7.7249365  -14.014837   -15.132178    29.537832\n","  -44.357845   -25.10781    -30.034399     4.5906124    4.302001\n","   42.40202     24.047504    30.647173    -0.5522032  -23.980118\n","  -15.658978     6.931843   -13.902735     4.3211513  -16.380178\n","   25.291079     7.215267   -40.175697    11.881879    17.884935\n","   21.105898    33.985645    10.213499   -30.883154   -34.022804  ]], shape=(1, 100), dtype=float32) tf.Tensor(\n","[[  4.433806   -18.988253     2.1529639  -27.326668    49.713825\n","    8.682512   -23.273375    -1.9951397    2.4936998    8.973715\n","   -1.8599373   -7.1919036    6.7736783   -0.31087086 -20.957582\n","  -15.428749     5.641995    15.899895     4.4306254    4.4011173\n","   22.178625   -19.799906    12.889212    13.110418   -16.683638\n","   25.890774    -8.944234    38.36873     11.599834    24.117496\n","    5.535174    20.454689    -7.629512   -44.362247     2.812642\n","   17.627329    25.411484    -9.689577    -0.6449996  -14.713276\n","   10.510589   -18.300186    43.4736       2.4911458   25.024986\n","   24.903423    -5.1086154  -17.983559     5.059588     5.155422\n","   33.052914    -2.0787666   23.037249   -23.23032     -9.98892\n","    4.9209127  -16.598412   -20.035166    13.885508    26.902351\n","  -27.619762   -22.40248     11.063886   -22.364944    28.248974\n","    8.923137    -7.3000135    8.517365    10.287554   -18.400373\n","   -3.7286415    9.036218    13.212252    -4.510193    -3.724378\n","  -19.773037   -23.251541   -10.463605    -4.2782936  -23.41745\n","  -28.599413    19.905258    22.21214     12.42019    -23.976685\n","  -24.048433    21.297312   -12.996926     7.4063897    3.4215317\n","   12.269719     2.713758   -11.24654      3.3691242   -8.336759\n","   31.154093   -23.975452    29.447813    51.436382    24.468302  ]], shape=(1, 100), dtype=float32)\n","tf.Tensor(\n","[[  4.3926907   14.151424    -1.9002774   -0.46270925  39.862465\n","  -12.310465   -24.058168   -39.16721    -18.323545   -46.488625\n","    4.8151712    0.17132623   2.420598    24.266884   -13.024626\n","  -10.781031    13.771802    -1.7078574   12.900978    42.875908\n","   23.553925    40.96557    -18.924215    -2.3832812  -12.206565\n","   36.412518    -3.6568909   -8.023915   -67.19778     39.103855\n","   18.740416   -38.54097    -13.9436865   75.688576     3.7137718\n","   -4.9173336   -9.596952   -51.860767    14.096268    33.30558\n","   19.413284   -13.205629     6.707736    36.184677   -41.9899\n","   15.385056    -2.6935816   12.291708    15.182088     9.548736\n","    5.928026   -23.802536   -17.47857      0.6391175  -20.916782\n","   -7.8591037   14.895381    16.41262     -9.6782465   17.834246\n","    2.4763236    5.960144   -24.613583   -53.345455   -37.583317\n","    1.1589434   36.170567   -13.297893     4.4836907   15.237108\n","   25.219852     7.915139   -15.51729    -15.562369    33.511734\n","  -47.56272    -27.524857   -32.245007     6.724201     4.6932945\n","   44.973423    26.013975    32.287613    -0.95861244 -25.868513\n","  -15.658821     6.9165115  -16.524649     4.660085   -16.846409\n","   28.322027     8.318546   -42.808178    13.115592    20.154917\n","   23.192938    36.40457     11.359986   -32.8789     -37.24818   ]], shape=(1, 100), dtype=float32) tf.Tensor(\n","[[  5.187582   -21.703808     2.1570313  -30.157116    53.580685\n","    7.8973894  -25.304033    -2.0245817    2.502385    10.4774275\n","   -0.76834875  -7.8602805    6.831316    -0.14769718 -22.849585\n","  -15.682441     5.905586    16.276897     4.354587     5.466688\n","   25.535208   -19.92857     15.115747    14.479282   -18.507566\n","   29.848524   -10.779252    41.571175    13.092764    26.467731\n","    5.914599    21.823652    -8.608468   -47.765804     4.513576\n","   18.891592    28.246376    -9.202066    -1.5480043  -14.685728\n","   11.660054   -20.1898      47.635635     3.0412061   26.481342\n","   27.203989    -5.7780013  -19.833927     6.1572847    5.33681\n","   34.6787      -1.8871564   24.799658   -24.930752   -10.730146\n","    4.5116796  -16.122746   -21.786871    16.093359    29.732843\n","  -29.531109   -23.532013    12.4291115  -23.131907    31.6265\n","   10.079999    -7.893824     8.541488    10.1153755  -20.563982\n","   -4.7767334    9.373846    15.237194    -5.103838    -4.7693176\n","  -22.43069    -24.896683   -11.659947    -4.5480824  -25.813622\n","  -30.56319     21.165857    22.361341    12.169305   -25.347439\n","  -26.322357    22.377234   -13.428309     7.290274     3.145092\n","   12.889327     1.9283495  -10.96323      3.5227726   -9.208071\n","   34.7823     -25.424034    30.874664    55.61137     25.064894  ]], shape=(1, 100), dtype=float32)\n"]}]},{"cell_type":"code","source":["from google.colab.patches import cv2_imshow\n","import cv2\n","from matplotlib import image\n","from PIL import Image\n","from numpy import asarray\n","\n","image = cv2.imread('/content/drive/MyDrive/VAE/train/1.png')\n","#image = cv2.resize(image, (256,256), interpolation = Image.ANTIALIAS )  #cv2.INTER_AREA\n","cv2_imshow(image)\n","image = asarray(image)\n","\n","image = np.expand_dims(image, axis=0)\n","#print(img)\n","z_mean,z_log_var = vae.encode(image)\n","#print(z_mean,z_log_var)\n","z = vae.reparameterize(z_mean, z_log_var)\n","#z = vae.sampling(z_mean, z_log_var)\n","\n","x_recon = vae.decode(z)\n","x_recon = x_recon.numpy()\n","#print(x_recon.shape)\n","x_recon = np.squeeze(x_recon)\n","def map1(x):\n","  return x*255.0\n","def map2(x):\n","  return tf.image.resize(x, (img_height, img_width))\n","def denormalize(x):\n","    x = 0.5 * (x + 1.0)\n","    x = x * 255.0\n","    x = tf.cast(x, tf.float32)\n","    return x\n","\n","x_recon = np.vectorize(denormalize)(x_recon)\n","#x_recon = np.vectorize(map2)(x_recon)\n","#x_recon = x_recon.map(lambda x: x*255.0)\n","#x_recon = x_recon.map(lambda x: tf.image.resize(x, (img_height, img_width)))\n","cv2_imshow(x_recon)\n","#for step, x in enumerate(train_ds):\n"," #  z_mean, z_log_var = vae.encode(x)\n"," #  print(len(z_mean),len(z_log_var))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":773},"id":"-eKX8_888cux","executionInfo":{"status":"error","timestamp":1677485659057,"user_tz":-120,"elapsed":804,"user":{"displayName":"Danu intern","userId":"14685699878604878640"}},"outputId":"3419b08e-396c-444e-982c-3dd71a9e6642"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=512x512 at 0x7F7868599E20>"],"image/png":"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\n"},"metadata":{}},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-19f9bffddc1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#print(img)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz_log_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m#print(z_mean,z_log_var)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'encode'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"20AzI4Mg6wyh"},"execution_count":null,"outputs":[]}]}